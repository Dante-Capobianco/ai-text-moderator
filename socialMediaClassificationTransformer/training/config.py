#Store hyperparameters (learning rate, batch size, epochs, number of layers)
DATA_PATH = "data/datasets/"
TOKENIZER_PATH = "data/tokenizer.py"
SAVED_MODEL_PATH = "trained_models/"
#Number of text inputs
BATCH_SIZE=256
DIMENSION=512
MAX_TOKEN_SIZE=500
LEARNING_RATE=0.1
VOCAB_SIZE=30522
EPOCHS=10
ATTENTION_HEADS=8
DROPOUT_RATE=0.1
